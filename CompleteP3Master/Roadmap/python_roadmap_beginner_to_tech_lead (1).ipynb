{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ Python Roadmap: Beginner to Tech Lead\n",
    "\n",
    "## 12-Month Comprehensive Guide\n",
    "\n",
    "A complete learning path from Python fundamentals to senior engineering and tech leadership.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Table of Contents\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŸ¢ BEGINNER (Months 1-2)\n",
    "*Master the core building blocks of Python and basic control flow.*\n",
    "\n",
    "| # | Topic | Description |\n",
    "|---|-------|-------------|\n",
    "| 1.1 | [Environment & Setup](#phase-1-fundamentals) | Python installation, virtual environments, IDE, package managers |\n",
    "| 1.2 | [Syntax & Data Types](#phase-1-fundamentals) | Variables, strings, integers, floats, booleans, None |\n",
    "| 1.3 | [Control Flow](#phase-1-fundamentals) | if/elif/else, for loops, while loops, match statements |\n",
    "| 1.4 | [Data Structures](#phase-1-fundamentals) | Lists, tuples, sets, dictionaries, collections module |\n",
    "| 1.5 | [Basic I/O & Modules](#phase-2-intermediate) | File handling, importing modules, standard library (os, sys, json) |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”µ MID-LEVEL DEVELOPER (Months 3-6)\n",
    "*Build structured applications and understand core programming paradigms.*\n",
    "\n",
    "| # | Topic | Description |\n",
    "|---|-------|-------------|\n",
    "| 2.1 | [Functions Deep Dive](#phase-2-intermediate) | Parameters, *args/**kwargs, lambda, closures, type hints, docstrings |\n",
    "| 2.2 | [Object-Oriented Programming](#phase-2-intermediate) | Classes, objects, inheritance, polymorphism, encapsulation, ABC |\n",
    "| 2.3 | [Error Handling & Logging](#phase-2-intermediate) | try/except, custom exceptions, context managers, structured logging |\n",
    "| 2.4 | [Testing Fundamentals](#phase-2-intermediate) | pytest, fixtures, parametrize, mocking, coverage |\n",
    "| 3.1 | [Decorators & Metaprogramming](#phase-3-advanced) | Function/class decorators, functools, metaclasses |\n",
    "| 3.2 | [Iterators & Generators](#phase-3-advanced) | Iterator protocol, yield, generator pipelines, itertools |\n",
    "| 4.1 | [Web Frameworks](#phase-4-web-apis) | FastAPI, Flask, Django basics |\n",
    "| 4.2 | [Databases & ORM](#phase-4-web-apis) | SQLAlchemy 2.0, Alembic, connection pooling |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŸ  SENIOR DEVELOPER (Months 7-10)\n",
    "*Handle complex system design, optimize performance, and understand Python internals.*\n",
    "\n",
    "| # | Topic | Description |\n",
    "|---|-------|-------------|\n",
    "| 3.3 | [ğŸ”¥ Threading & Concurrency](#phase-3-advanced) | Thread, Lock, RLock, Semaphore, race conditions, thread safety |\n",
    "| 3.4 | [ğŸ”¥ Multiprocessing](#phase-3-advanced) | Process, Pool, Queue, Pipe, shared memory (Value, Array), Manager |\n",
    "| 3.5 | [ğŸ”¥ GIL (Global Interpreter Lock)](#phase-3-advanced) | Understanding GIL limitations and workarounds |\n",
    "| 3.6 | [ğŸ”¥ Asyncio Framework](#phase-3-advanced) | async/await, event loops, tasks, gather, AsyncIO patterns |\n",
    "| 3.7 | [Advanced Pythonic Concepts](#phase-3-advanced) | Decorators, generators, context managers, metaclasses mastery |\n",
    "| 5.1 | [Data Processing at Scale](#phase-5-data-ml) | Pandas optimization, Polars, Dask/Ray |\n",
    "| 5.2 | [ML Pipeline & MLOps](#phase-5-data-ml) | Scikit-learn pipelines, MLflow, model serving |\n",
    "| 6.1 | [ğŸ”¥ Caching Strategies](#phase-6-performance) | In-memory cache, Redis, cache-aside, write-through, TTL, LRU |\n",
    "| 6.2 | [ğŸ”¥ Profiling & Optimization](#phase-6-performance) | cProfile, line_profiler, memory_profiler, __slots__ |\n",
    "| 7.1 | [Design Patterns](#phase-7-system-design) | Singleton, Factory, Strategy, Observer, Repository, CQRS |\n",
    "| 7.2 | [Message Queues & Events](#phase-7-system-design) | Celery, RabbitMQ, Kafka, event-driven architecture |\n",
    "| 8.1 | [Docker & Kubernetes](#phase-8-production) | Multi-stage builds, orchestration, Helm |\n",
    "| 8.2 | [CI/CD Pipelines](#phase-8-production) | GitHub Actions, testing pipelines, deployment |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”´ TECH LEAD (Months 11-12)\n",
    "*Blend deep technical expertise with leadership and strategic thinking.*\n",
    "\n",
    "| # | Topic | Description |\n",
    "|---|-------|-------------|\n",
    "| 7.3 | [ğŸ”¥ Software Design & Architecture](#phase-7-system-design) | Scalable systems, architectural decisions, trade-offs |\n",
    "| 6.3 | [ğŸ”¥ Performance & Memory Management](#phase-6-performance) | Profiling bottlenecks, memory leaks, large-scale data |\n",
    "| 8.3 | [Monitoring & Observability](#phase-8-production) | Prometheus, Grafana, OpenTelemetry, Sentry |\n",
    "| 9.1 | [ğŸ”¥ Code Review Best Practices](#phase-9-leadership) | Review checklists, style guides, feedback techniques |\n",
    "| 9.2 | [ğŸ”¥ Architecture Decision Records](#phase-9-leadership) | ADR format, documenting decisions, trade-off analysis |\n",
    "| 9.3 | [ğŸ”¥ Technical Leadership](#phase-9-leadership) | Mentoring, project estimation, stakeholder communication |\n",
    "| 9.4 | [System Design Interviews](#phase-9-leadership) | Whiteboard design, capacity planning, scaling strategies |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Quick Reference: Essential Topics by Level\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  BEGINNER                                                                    â”‚\n",
    "â”‚  â€¢ Syntax and Data Types    â€¢ Control Flow    â€¢ Basic I/O and Modules       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  MID-LEVEL DEVELOPER                                                         â”‚\n",
    "â”‚  â€¢ Object-Oriented Programming (OOP)                                         â”‚\n",
    "â”‚  â€¢ Data Structures and Algorithms                                            â”‚\n",
    "â”‚  â€¢ Version Control (Git) and Testing (pytest)                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  SENIOR DEVELOPER                                                            â”‚\n",
    "â”‚  â€¢ Concurrency: Threading, Multiprocessing, GIL, asyncio                     â”‚\n",
    "â”‚  â€¢ Advanced: Decorators, Generators, Context Managers, Metaclasses           â”‚\n",
    "â”‚  â€¢ System: Databases, APIs (FastAPI/Django), Docker, CI/CD                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  TECH LEAD                                                                   â”‚\n",
    "â”‚  â€¢ Software Design and Architecture                                          â”‚\n",
    "â”‚  â€¢ Performance Optimization and Memory Management                            â”‚\n",
    "â”‚  â€¢ Technical Leadership and Mentorship                                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Skill Level Overview\n",
    "\n",
    "| Level | Months | Focus | Outcome |\n",
    "|-------|--------|-------|----------|\n",
    "| **Junior** | 1-4 | Fundamentals, OOP, Testing | Write clean, tested code |\n",
    "| **Mid-Level** | 5-8 | Web, APIs, Databases, ML | Build production applications |\n",
    "| **Senior** | 9-10 | Performance, Architecture, Scale | Design scalable systems |\n",
    "| **Tech Lead** | 11-12 | Leadership, Strategy, Mentoring | Lead teams & technical decisions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='phase-1-fundamentals'></a>\n",
    "# ğŸ“˜ Phase 1: Python Fundamentals (Month 1-2)\n",
    "\n",
    "**Goal**: Master Python basics and write clean, readable code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Environment & Setup\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Python 3.11+ | Installation & versions | â¬œ |\n",
    "| Virtual Environments | venv, conda, pyenv | â¬œ |\n",
    "| IDE Setup | VS Code, PyCharm | â¬œ |\n",
    "| Package Managers | pip, poetry, uv | â¬œ |\n",
    "| pyproject.toml | Modern project config | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup commands\n",
    "import sys\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "# Virtual environment creation\n",
    "# python -m venv .venv\n",
    "# source .venv/bin/activate  # Linux/Mac\n",
    "# .venv\\Scripts\\activate     # Windows\n",
    "\n",
    "# Modern package management with poetry\n",
    "# poetry init\n",
    "# poetry add requests pandas\n",
    "# poetry install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Core Syntax & Data Types\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Variables | Assignment, naming conventions | â¬œ |\n",
    "| Data Types | int, float, str, bool, None | â¬œ |\n",
    "| Operators | arithmetic, comparison, logical, bitwise | â¬œ |\n",
    "| String Formatting | f-strings, .format() | â¬œ |\n",
    "| Type Conversion | int(), str(), float() | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core syntax examples\n",
    "name: str = \"Python\"\n",
    "version: float = 3.11\n",
    "is_awesome: bool = True\n",
    "\n",
    "# F-strings with formatting\n",
    "print(f\"Language: {name}, Version: {version:.1f}\")\n",
    "\n",
    "# Operators\n",
    "a, b = 17, 5\n",
    "print(f\"Floor division: {a // b}, Modulo: {a % b}, Power: {a ** b}\")\n",
    "\n",
    "# Bitwise (important for systems programming)\n",
    "print(f\"AND: {a & b}, OR: {a | b}, XOR: {a ^ b}, Left Shift: {a << 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Control Flow\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Conditionals | if/elif/else, ternary | â¬œ |\n",
    "| Loops | for, while, break, continue | â¬œ |\n",
    "| Match Statements | Python 3.10+ structural pattern matching | â¬œ |\n",
    "| Comprehensions | list, dict, set, generator | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern matching (Python 3.10+)\n",
    "def handle_command(command: dict):\n",
    "    match command:\n",
    "        case {\"action\": \"start\", \"target\": target}:\n",
    "            return f\"Starting {target}\"\n",
    "        case {\"action\": \"stop\", \"target\": target, \"force\": True}:\n",
    "            return f\"Force stopping {target}\"\n",
    "        case {\"action\": \"stop\", \"target\": target}:\n",
    "            return f\"Gracefully stopping {target}\"\n",
    "        case _:\n",
    "            return \"Unknown command\"\n",
    "\n",
    "print(handle_command({\"action\": \"start\", \"target\": \"server\"}))\n",
    "print(handle_command({\"action\": \"stop\", \"target\": \"db\", \"force\": True}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced comprehensions\n",
    "# Nested comprehension\n",
    "matrix = [[i * j for j in range(1, 4)] for i in range(1, 4)]\n",
    "print(f\"Matrix: {matrix}\")\n",
    "\n",
    "# Flattening with comprehension\n",
    "flat = [item for row in matrix for item in row]\n",
    "print(f\"Flattened: {flat}\")\n",
    "\n",
    "# Dict comprehension with condition\n",
    "squares = {x: x**2 for x in range(10) if x % 2 == 0}\n",
    "print(f\"Even squares: {squares}\")\n",
    "\n",
    "# Walrus operator in comprehension\n",
    "results = [y for x in range(10) if (y := x**2) > 20]\n",
    "print(f\"Squares > 20: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Data Structures\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Lists | Ordered, mutable | â¬œ |\n",
    "| Tuples | Ordered, immutable | â¬œ |\n",
    "| Sets | Unordered, unique | â¬œ |\n",
    "| Dictionaries | Key-value pairs | â¬œ |\n",
    "| Collections Module | Counter, defaultdict, deque, namedtuple | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict, deque, namedtuple, OrderedDict\n",
    "\n",
    "# Counter - frequency counting\n",
    "words = [\"apple\", \"banana\", \"apple\", \"cherry\", \"banana\", \"apple\"]\n",
    "word_count = Counter(words)\n",
    "print(f\"Most common: {word_count.most_common(2)}\")\n",
    "\n",
    "# defaultdict - auto-initialize missing keys\n",
    "graph = defaultdict(list)\n",
    "graph[\"A\"].append(\"B\")\n",
    "graph[\"A\"].append(\"C\")\n",
    "print(f\"Graph: {dict(graph)}\")\n",
    "\n",
    "# deque - O(1) operations on both ends\n",
    "queue = deque(maxlen=3)\n",
    "queue.append(1)\n",
    "queue.append(2)\n",
    "queue.appendleft(0)\n",
    "print(f\"Deque: {queue}\")\n",
    "\n",
    "# namedtuple - lightweight objects\n",
    "Point = namedtuple('Point', ['x', 'y'])\n",
    "p = Point(10, 20)\n",
    "print(f\"Point: {p.x}, {p.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='phase-2-intermediate'></a>\n",
    "# ğŸ“— Phase 2: Intermediate Python (Month 3-4)\n",
    "\n",
    "**Goal**: Master functions, OOP, and professional Python patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Functions Deep Dive\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Parameters | positional, keyword, default | â¬œ |\n",
    "| *args, **kwargs | Variable arguments | â¬œ |\n",
    "| Lambda & Higher-Order | Anonymous functions | â¬œ |\n",
    "| Closures | Function factories | â¬œ |\n",
    "| Type Hints | Full typing module | â¬œ |\n",
    "| Docstrings | Google/NumPy style | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, TypeVar, ParamSpec\n",
    "\n",
    "T = TypeVar('T')\n",
    "P = ParamSpec('P')\n",
    "\n",
    "def retry(times: int) -> Callable[[Callable[P, T]], Callable[P, T]]:\n",
    "    \"\"\"Decorator factory for retrying failed functions.\n",
    "    \n",
    "    Args:\n",
    "        times: Number of retry attempts.\n",
    "        \n",
    "    Returns:\n",
    "        Decorated function with retry logic.\n",
    "        \n",
    "    Example:\n",
    "        >>> @retry(times=3)\n",
    "        ... def fetch_data():\n",
    "        ...     pass\n",
    "    \"\"\"\n",
    "    def decorator(func: Callable[P, T]) -> Callable[P, T]:\n",
    "        def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\n",
    "            last_exception = None\n",
    "            for attempt in range(times):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    last_exception = e\n",
    "                    print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            raise last_exception\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@retry(times=3)\n",
    "def unstable_function():\n",
    "    import random\n",
    "    if random.random() < 0.7:\n",
    "        raise ValueError(\"Random failure\")\n",
    "    return \"Success!\"\n",
    "\n",
    "try:\n",
    "    result = unstable_function()\n",
    "    print(result)\n",
    "except ValueError:\n",
    "    print(\"All attempts failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Object-Oriented Programming\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Classes & Objects | Basic OOP | â¬œ |\n",
    "| Inheritance | Single, Multiple, MRO | â¬œ |\n",
    "| Magic Methods | __init__, __str__, __eq__, etc. | â¬œ |\n",
    "| Properties | @property, @setter, @deleter | â¬œ |\n",
    "| Abstract Classes | ABC, abstractmethod | â¬œ |\n",
    "| Dataclasses | @dataclass, field() | â¬œ |\n",
    "| Protocols | Structural subtyping | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Protocol, List, Optional\n",
    "from functools import total_ordering\n",
    "\n",
    "# Abstract Base Class\n",
    "class Repository(ABC):\n",
    "    @abstractmethod\n",
    "    def get(self, id: str) -> dict:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save(self, entity: dict) -> None:\n",
    "        pass\n",
    "\n",
    "# Protocol for duck typing\n",
    "class Cacheable(Protocol):\n",
    "    def cache_key(self) -> str: ...\n",
    "    def ttl(self) -> int: ...\n",
    "\n",
    "# Dataclass with advanced features\n",
    "@total_ordering\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class Money:\n",
    "    amount: float\n",
    "    currency: str = \"USD\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.amount < 0:\n",
    "            raise ValueError(\"Amount cannot be negative\")\n",
    "    \n",
    "    def __add__(self, other: 'Money') -> 'Money':\n",
    "        if self.currency != other.currency:\n",
    "            raise ValueError(\"Cannot add different currencies\")\n",
    "        return Money(self.amount + other.amount, self.currency)\n",
    "    \n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        if not isinstance(other, Money):\n",
    "            return NotImplemented\n",
    "        return self.amount == other.amount and self.currency == other.currency\n",
    "    \n",
    "    def __lt__(self, other: 'Money') -> bool:\n",
    "        if self.currency != other.currency:\n",
    "            raise ValueError(\"Cannot compare different currencies\")\n",
    "        return self.amount < other.amount\n",
    "\n",
    "m1 = Money(100)\n",
    "m2 = Money(50)\n",
    "print(f\"Sum: {m1 + m2}\")\n",
    "print(f\"m1 > m2: {m1 > m2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Error Handling & Logging\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| try/except/else/finally | Exception handling | â¬œ |\n",
    "| Custom Exceptions | Exception hierarchies | â¬œ |\n",
    "| Context Managers | __enter__, __exit__ | â¬œ |\n",
    "| Logging | Handlers, formatters, levels | â¬œ |\n",
    "| Structured Logging | JSON logging | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom exception hierarchy\n",
    "class AppError(Exception):\n",
    "    \"\"\"Base application error.\"\"\"\n",
    "    def __init__(self, message: str, code: str = None, details: dict = None):\n",
    "        super().__init__(message)\n",
    "        self.code = code\n",
    "        self.details = details or {}\n",
    "        self.timestamp = datetime.utcnow()\n",
    "\n",
    "class ValidationError(AppError):\n",
    "    \"\"\"Data validation error.\"\"\"\n",
    "    pass\n",
    "\n",
    "class DatabaseError(AppError):\n",
    "    \"\"\"Database operation error.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Structured JSON logging\n",
    "class JSONFormatter(logging.Formatter):\n",
    "    def format(self, record):\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"level\": record.levelname,\n",
    "            \"message\": record.getMessage(),\n",
    "            \"module\": record.module,\n",
    "            \"function\": record.funcName,\n",
    "        }\n",
    "        if hasattr(record, 'extra'):\n",
    "            log_entry.update(record.extra)\n",
    "        return json.dumps(log_entry)\n",
    "\n",
    "# Setup structured logger\n",
    "logger = logging.getLogger(\"app\")\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(JSONFormatter())\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info(\"Application started\", extra={\"extra\": {\"version\": \"1.0.0\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Testing Fundamentals\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| pytest | Test discovery, assertions | â¬œ |\n",
    "| Fixtures | Setup/teardown | â¬œ |\n",
    "| Parametrize | Data-driven tests | â¬œ |\n",
    "| Mocking | unittest.mock, pytest-mock | â¬œ |\n",
    "| Coverage | pytest-cov | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytest example (save as test_example.py)\n",
    "test_code = '''\n",
    "import pytest\n",
    "from unittest.mock import Mock, patch, AsyncMock\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class User:\n",
    "    id: int\n",
    "    name: str\n",
    "    email: str\n",
    "\n",
    "class UserService:\n",
    "    def __init__(self, repository, email_client):\n",
    "        self.repository = repository\n",
    "        self.email_client = email_client\n",
    "    \n",
    "    def create_user(self, name: str, email: str) -> User:\n",
    "        user = User(id=1, name=name, email=email)\n",
    "        self.repository.save(user)\n",
    "        self.email_client.send_welcome(email)\n",
    "        return user\n",
    "\n",
    "class TestUserService:\n",
    "    @pytest.fixture\n",
    "    def mock_repository(self):\n",
    "        return Mock()\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def mock_email_client(self):\n",
    "        return Mock()\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def service(self, mock_repository, mock_email_client):\n",
    "        return UserService(mock_repository, mock_email_client)\n",
    "    \n",
    "    def test_create_user_saves_to_repository(self, service, mock_repository):\n",
    "        user = service.create_user(\"John\", \"john@email.com\")\n",
    "        mock_repository.save.assert_called_once()\n",
    "    \n",
    "    def test_create_user_sends_welcome_email(self, service, mock_email_client):\n",
    "        user = service.create_user(\"John\", \"john@email.com\")\n",
    "        mock_email_client.send_welcome.assert_called_once_with(\"john@email.com\")\n",
    "    \n",
    "    @pytest.mark.parametrize(\"name,email\", [\n",
    "        (\"Alice\", \"alice@test.com\"),\n",
    "        (\"Bob\", \"bob@test.com\"),\n",
    "    ])\n",
    "    def test_create_user_returns_correct_data(self, service, name, email):\n",
    "        user = service.create_user(name, email)\n",
    "        assert user.name == name\n",
    "        assert user.email == email\n",
    "'''\n",
    "\n",
    "print(\"pytest Example with Fixtures and Mocking:\")\n",
    "print(test_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='phase-3-advanced'></a>\n",
    "# ğŸ“™ Phase 3: Advanced Python (Month 5-6)\n",
    "\n",
    "**Goal**: Master advanced Python features for production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Decorators & Metaprogramming\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Function Decorators | Modifying behavior | â¬œ |\n",
    "| Class Decorators | Modifying classes | â¬œ |\n",
    "| Decorator Stacking | Multiple decorators | â¬œ |\n",
    "| functools | wraps, lru_cache, cached_property | â¬œ |\n",
    "| Metaclasses | Type creation | â¬œ |\n",
    "| __init_subclass__ | Subclass hooks | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "from typing import Callable, Any\n",
    "\n",
    "# Production-ready decorator with caching\n",
    "def memoize(maxsize: int = 128, typed: bool = False):\n",
    "    \"\"\"LRU cache decorator with statistics.\"\"\"\n",
    "    def decorator(func: Callable) -> Callable:\n",
    "        cache = functools.lru_cache(maxsize=maxsize, typed=typed)(func)\n",
    "        \n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            return cache(*args, **kwargs)\n",
    "        \n",
    "        wrapper.cache_info = cache.cache_info\n",
    "        wrapper.cache_clear = cache.cache_clear\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@memoize(maxsize=100)\n",
    "def fibonacci(n: int) -> int:\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fibonacci(n - 1) + fibonacci(n - 2)\n",
    "\n",
    "print(f\"fib(30) = {fibonacci(30)}\")\n",
    "print(f\"Cache stats: {fibonacci.cache_info()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metaclass example - Plugin registry\n",
    "class PluginRegistry(type):\n",
    "    \"\"\"Metaclass that auto-registers plugins.\"\"\"\n",
    "    plugins: dict = {}\n",
    "    \n",
    "    def __new__(mcs, name, bases, namespace):\n",
    "        cls = super().__new__(mcs, name, bases, namespace)\n",
    "        if bases:  # Don't register the base class\n",
    "            plugin_name = namespace.get('name', name.lower())\n",
    "            mcs.plugins[plugin_name] = cls\n",
    "        return cls\n",
    "\n",
    "class Plugin(metaclass=PluginRegistry):\n",
    "    \"\"\"Base plugin class.\"\"\"\n",
    "    def execute(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class EmailPlugin(Plugin):\n",
    "    name = \"email\"\n",
    "    def execute(self):\n",
    "        return \"Sending email...\"\n",
    "\n",
    "class SlackPlugin(Plugin):\n",
    "    name = \"slack\"\n",
    "    def execute(self):\n",
    "        return \"Sending Slack message...\"\n",
    "\n",
    "print(f\"Registered plugins: {list(PluginRegistry.plugins.keys())}\")\n",
    "print(f\"Execute email: {PluginRegistry.plugins['email']().execute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Iterators & Generators\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Iterator Protocol | __iter__, __next__ | â¬œ |\n",
    "| Generators | yield, yield from | â¬œ |\n",
    "| Generator Pipelines | Data processing | â¬œ |\n",
    "| itertools | Advanced iteration | â¬œ |\n",
    "| Coroutines | Generator-based coroutines | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice, chain, groupby, tee\n",
    "from typing import Iterator, Iterable, TypeVar\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "def chunked(iterable: Iterable[T], size: int) -> Iterator[list[T]]:\n",
    "    \"\"\"Split iterable into chunks of given size.\"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while chunk := list(islice(iterator, size)):\n",
    "        yield chunk\n",
    "\n",
    "def sliding_window(iterable: Iterable[T], size: int) -> Iterator[tuple[T, ...]]:\n",
    "    \"\"\"Generate sliding windows over iterable.\"\"\"\n",
    "    iterators = tee(iterable, size)\n",
    "    for i, it in enumerate(iterators):\n",
    "        for _ in range(i):\n",
    "            next(it, None)\n",
    "    return zip(*iterators)\n",
    "\n",
    "# Generator pipeline for data processing\n",
    "def read_logs(filename: str) -> Iterator[str]:\n",
    "    \"\"\"Simulate reading log lines.\"\"\"\n",
    "    logs = [\n",
    "        \"2024-01-01 ERROR: Connection failed\",\n",
    "        \"2024-01-01 INFO: Server started\",\n",
    "        \"2024-01-01 ERROR: Timeout\",\n",
    "        \"2024-01-02 INFO: Request processed\",\n",
    "    ]\n",
    "    yield from logs\n",
    "\n",
    "def filter_errors(lines: Iterator[str]) -> Iterator[str]:\n",
    "    return (line for line in lines if \"ERROR\" in line)\n",
    "\n",
    "def extract_message(lines: Iterator[str]) -> Iterator[str]:\n",
    "    return (line.split(\": \", 1)[1] for line in lines)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = extract_message(filter_errors(read_logs(\"app.log\")))\n",
    "print(f\"Errors: {list(pipeline)}\")\n",
    "\n",
    "# Chunked processing\n",
    "data = range(10)\n",
    "print(f\"Chunks: {list(chunked(data, 3))}\")\n",
    "print(f\"Windows: {list(sliding_window(range(5), 3))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 ğŸ”¥ Threading & Concurrency (Senior Level)\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| GIL | Global Interpreter Lock understanding | â¬œ |\n",
    "| Threading | Thread, Lock, RLock, Semaphore | â¬œ |\n",
    "| Race Conditions | Thread safety | â¬œ |\n",
    "| Thread Pools | ThreadPoolExecutor | â¬œ |\n",
    "| Multiprocessing | Process, Pool, shared memory | â¬œ |\n",
    "| Process Communication | Queue, Pipe, Manager | â¬œ |\n",
    "| Asyncio | Event loop, tasks, async/await | â¬œ |\n",
    "| Async Patterns | gather, wait, create_task | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from queue import Queue\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Thread-safe counter with Lock\n",
    "class ThreadSafeCounter:\n",
    "    def __init__(self):\n",
    "        self._value = 0\n",
    "        self._lock = threading.Lock()\n",
    "    \n",
    "    def increment(self):\n",
    "        with self._lock:\n",
    "            self._value += 1\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        with self._lock:\n",
    "            return self._value\n",
    "\n",
    "# Rate limiter using Semaphore\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_concurrent: int):\n",
    "        self._semaphore = threading.Semaphore(max_concurrent)\n",
    "    \n",
    "    @contextmanager\n",
    "    def acquire(self):\n",
    "        self._semaphore.acquire()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self._semaphore.release()\n",
    "\n",
    "# Producer-Consumer pattern\n",
    "class WorkerPool:\n",
    "    def __init__(self, num_workers: int):\n",
    "        self.queue = Queue()\n",
    "        self.workers = []\n",
    "        self.results = []\n",
    "        self._lock = threading.Lock()\n",
    "        \n",
    "        for _ in range(num_workers):\n",
    "            worker = threading.Thread(target=self._worker, daemon=True)\n",
    "            worker.start()\n",
    "            self.workers.append(worker)\n",
    "    \n",
    "    def _worker(self):\n",
    "        while True:\n",
    "            task = self.queue.get()\n",
    "            if task is None:\n",
    "                break\n",
    "            func, args = task\n",
    "            result = func(*args)\n",
    "            with self._lock:\n",
    "                self.results.append(result)\n",
    "            self.queue.task_done()\n",
    "    \n",
    "    def submit(self, func, *args):\n",
    "        self.queue.put((func, args))\n",
    "    \n",
    "    def wait(self):\n",
    "        self.queue.join()\n",
    "\n",
    "# Demo\n",
    "counter = ThreadSafeCounter()\n",
    "limiter = RateLimiter(max_concurrent=3)\n",
    "\n",
    "def task(n):\n",
    "    with limiter.acquire():\n",
    "        counter.increment()\n",
    "        time.sleep(0.1)\n",
    "        return n ** 2\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(task, i) for i in range(10)]\n",
    "    results = [f.result() for f in as_completed(futures)]\n",
    "\n",
    "print(f\"Counter: {counter.value}\")\n",
    "print(f\"Results: {sorted(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced threading - ReadWriteLock\n",
    "class ReadWriteLock:\n",
    "    \"\"\"Lock that allows multiple readers but only one writer.\"\"\"\n",
    "    def __init__(self):\n",
    "        self._read_ready = threading.Condition(threading.Lock())\n",
    "        self._readers = 0\n",
    "    \n",
    "    @contextmanager\n",
    "    def read_lock(self):\n",
    "        with self._read_ready:\n",
    "            self._readers += 1\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            with self._read_ready:\n",
    "                self._readers -= 1\n",
    "                if self._readers == 0:\n",
    "                    self._read_ready.notify_all()\n",
    "    \n",
    "    @contextmanager\n",
    "    def write_lock(self):\n",
    "        with self._read_ready:\n",
    "            while self._readers > 0:\n",
    "                self._read_ready.wait()\n",
    "            yield\n",
    "\n",
    "# Thread-local storage\n",
    "class RequestContext:\n",
    "    \"\"\"Thread-local request context.\"\"\"\n",
    "    _local = threading.local()\n",
    "    \n",
    "    @classmethod\n",
    "    def set_request_id(cls, request_id: str):\n",
    "        cls._local.request_id = request_id\n",
    "    \n",
    "    @classmethod\n",
    "    def get_request_id(cls) -> str:\n",
    "        return getattr(cls._local, 'request_id', 'unknown')\n",
    "\n",
    "print(\"ReadWriteLock and ThreadLocal defined for concurrent access patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asyncio - Production patterns\n",
    "import asyncio\n",
    "from typing import List, Any\n",
    "\n",
    "# Async semaphore for rate limiting\n",
    "class AsyncRateLimiter:\n",
    "    def __init__(self, rate: int, per_seconds: float = 1.0):\n",
    "        self.rate = rate\n",
    "        self.per_seconds = per_seconds\n",
    "        self._semaphore = asyncio.Semaphore(rate)\n",
    "        self._tokens = rate\n",
    "        self._updated_at = time.monotonic()\n",
    "        self._lock = asyncio.Lock()\n",
    "    \n",
    "    async def acquire(self):\n",
    "        async with self._lock:\n",
    "            now = time.monotonic()\n",
    "            time_passed = now - self._updated_at\n",
    "            self._tokens = min(self.rate, self._tokens + time_passed * (self.rate / self.per_seconds))\n",
    "            self._updated_at = now\n",
    "            \n",
    "            if self._tokens < 1:\n",
    "                wait_time = (1 - self._tokens) * (self.per_seconds / self.rate)\n",
    "                await asyncio.sleep(wait_time)\n",
    "                self._tokens = 1\n",
    "            \n",
    "            self._tokens -= 1\n",
    "\n",
    "# Async retry with exponential backoff\n",
    "async def async_retry(\n",
    "    func,\n",
    "    retries: int = 3,\n",
    "    backoff_factor: float = 2.0,\n",
    "    initial_delay: float = 1.0\n",
    "):\n",
    "    delay = initial_delay\n",
    "    last_exception = None\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return await func()\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            if attempt < retries - 1:\n",
    "                await asyncio.sleep(delay)\n",
    "                delay *= backoff_factor\n",
    "    \n",
    "    raise last_exception\n",
    "\n",
    "# Async context manager\n",
    "class AsyncDatabaseConnection:\n",
    "    async def __aenter__(self):\n",
    "        print(\"Connecting to database...\")\n",
    "        await asyncio.sleep(0.1)  # Simulate connection\n",
    "        return self\n",
    "    \n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "        print(\"Closing database connection...\")\n",
    "        await asyncio.sleep(0.05)  # Simulate cleanup\n",
    "        return False\n",
    "    \n",
    "    async def query(self, sql: str):\n",
    "        await asyncio.sleep(0.05)  # Simulate query\n",
    "        return [{\"id\": 1, \"name\": \"Test\"}]\n",
    "\n",
    "# Batch processing with concurrency limit\n",
    "async def process_batch(\n",
    "    items: List[Any],\n",
    "    processor,\n",
    "    max_concurrent: int = 10\n",
    ") -> List[Any]:\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def process_with_limit(item):\n",
    "        async with semaphore:\n",
    "            return await processor(item)\n",
    "    \n",
    "    return await asyncio.gather(\n",
    "        *[process_with_limit(item) for item in items],\n",
    "        return_exceptions=True\n",
    "    )\n",
    "\n",
    "# Demo\n",
    "async def demo():\n",
    "    async with AsyncDatabaseConnection() as db:\n",
    "        result = await db.query(\"SELECT * FROM users\")\n",
    "        print(f\"Query result: {result}\")\n",
    "\n",
    "await demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Multiprocessing (CPU-Bound Tasks)\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Process | Creating processes | â¬œ |\n",
    "| Pool | Process pools | â¬œ |\n",
    "| Shared Memory | Value, Array | â¬œ |\n",
    "| Inter-process Communication | Queue, Pipe | â¬œ |\n",
    "| Manager | Shared objects | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, Process, Queue, Manager, Value, Array\n",
    "import os\n",
    "\n",
    "# CPU-bound task\n",
    "def cpu_intensive_task(n: int) -> int:\n",
    "    \"\"\"Simulate CPU-bound work.\"\"\"\n",
    "    result = 0\n",
    "    for i in range(n * 1000):\n",
    "        result += i ** 2\n",
    "    return result\n",
    "\n",
    "# Using Pool for parallel execution\n",
    "def parallel_map_example():\n",
    "    with Pool(processes=4) as pool:\n",
    "        results = pool.map(cpu_intensive_task, range(10))\n",
    "    return results\n",
    "\n",
    "# Shared memory example\n",
    "def shared_memory_example():\n",
    "    # Shared counter\n",
    "    counter = Value('i', 0)  # 'i' = integer\n",
    "    \n",
    "    # Shared array\n",
    "    arr = Array('d', [0.0] * 10)  # 'd' = double\n",
    "    \n",
    "    def worker(shared_counter, shared_array, index):\n",
    "        with shared_counter.get_lock():\n",
    "            shared_counter.value += 1\n",
    "        shared_array[index] = index ** 2\n",
    "    \n",
    "    processes = []\n",
    "    for i in range(10):\n",
    "        p = Process(target=worker, args=(counter, arr, i))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    \n",
    "    return counter.value, list(arr)\n",
    "\n",
    "# Manager for complex shared objects\n",
    "def manager_example():\n",
    "    with Manager() as manager:\n",
    "        shared_dict = manager.dict()\n",
    "        shared_list = manager.list()\n",
    "        \n",
    "        def worker(d, l, key, value):\n",
    "            d[key] = value\n",
    "            l.append(value)\n",
    "        \n",
    "        processes = []\n",
    "        for i in range(5):\n",
    "            p = Process(target=worker, args=(shared_dict, shared_list, f'key{i}', i))\n",
    "            processes.append(p)\n",
    "            p.start()\n",
    "        \n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        \n",
    "        return dict(shared_dict), list(shared_list)\n",
    "\n",
    "print(f\"Pool result length: {len(parallel_map_example())}\")\n",
    "counter_val, arr_val = shared_memory_example()\n",
    "print(f\"Shared counter: {counter_val}, Array: {arr_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='phase-4-web-apis'></a>\n",
    "# ğŸ“• Phase 4: Web & APIs (Month 7)\n",
    "\n",
    "**Goal**: Build production-grade web applications and APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 FastAPI Production Setup\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Project Structure | Clean architecture | â¬œ |\n",
    "| Dependency Injection | Depends() | â¬œ |\n",
    "| Middleware | CORS, logging, auth | â¬œ |\n",
    "| Background Tasks | Task queues | â¬œ |\n",
    "| WebSockets | Real-time communication | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production FastAPI structure\n",
    "fastapi_structure = '''\n",
    "project/\n",
    "â”œâ”€â”€ app/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ main.py              # FastAPI app entry point\n",
    "â”‚   â”œâ”€â”€ config.py            # Settings and configuration\n",
    "â”‚   â”œâ”€â”€ dependencies.py      # Dependency injection\n",
    "â”‚   â”œâ”€â”€ api/\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ v1/\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ router.py    # API router\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ users.py     # User endpoints\n",
    "â”‚   â”‚   â”‚   â””â”€â”€ items.py     # Item endpoints\n",
    "â”‚   â”œâ”€â”€ core/\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ security.py      # Auth, JWT\n",
    "â”‚   â”‚   â””â”€â”€ middleware.py    # Custom middleware\n",
    "â”‚   â”œâ”€â”€ models/\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ user.py          # SQLAlchemy models\n",
    "â”‚   â”‚   â””â”€â”€ item.py\n",
    "â”‚   â”œâ”€â”€ schemas/\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ user.py          # Pydantic schemas\n",
    "â”‚   â”‚   â””â”€â”€ item.py\n",
    "â”‚   â”œâ”€â”€ services/\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ user_service.py  # Business logic\n",
    "â”‚   â”‚   â””â”€â”€ item_service.py\n",
    "â”‚   â””â”€â”€ repositories/\n",
    "â”‚       â”œâ”€â”€ __init__.py\n",
    "â”‚       â”œâ”€â”€ base.py          # Base repository\n",
    "â”‚       â””â”€â”€ user_repo.py     # Data access\n",
    "â”œâ”€â”€ tests/\n",
    "â”œâ”€â”€ alembic/                  # Migrations\n",
    "â”œâ”€â”€ docker-compose.yml\n",
    "â”œâ”€â”€ Dockerfile\n",
    "â””â”€â”€ pyproject.toml\n",
    "'''\n",
    "\n",
    "print(\"Production FastAPI Project Structure:\")\n",
    "print(fastapi_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI with dependency injection and middleware\n",
    "fastapi_code = '''\n",
    "from fastapi import FastAPI, Depends, HTTPException, Request, status\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.security import OAuth2PasswordBearer\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "from contextlib import asynccontextmanager\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Lifespan for startup/shutdown\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    # Startup\n",
    "    logging.info(\"Starting application...\")\n",
    "    yield\n",
    "    # Shutdown\n",
    "    logging.info(\"Shutting down...\")\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Production API\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "# CORS Middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"https://yourdomain.com\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Custom logging middleware\n",
    "@app.middleware(\"http\")\n",
    "async def log_requests(request: Request, call_next):\n",
    "    start_time = time.time()\n",
    "    response = await call_next(request)\n",
    "    duration = time.time() - start_time\n",
    "    logging.info(\n",
    "        f\"{request.method} {request.url.path} \"\n",
    "        f\"status={response.status_code} duration={duration:.3f}s\"\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Dependency injection\n",
    "class DatabaseSession:\n",
    "    async def __aenter__(self):\n",
    "        return self\n",
    "    async def __aexit__(self, *args):\n",
    "        pass\n",
    "\n",
    "async def get_db():\n",
    "    async with DatabaseSession() as session:\n",
    "        yield session\n",
    "\n",
    "async def get_current_user(token: str = Depends(OAuth2PasswordBearer(tokenUrl=\"token\"))):\n",
    "    # Validate token and return user\n",
    "    return {\"id\": 1, \"username\": \"user\"}\n",
    "\n",
    "# Schemas\n",
    "class UserCreate(BaseModel):\n",
    "    username: str = Field(..., min_length=3, max_length=50)\n",
    "    email: str = Field(..., pattern=r\"^[\\\\w.-]+@[\\\\w.-]+\\\\.\\\\w+$\")\n",
    "    password: str = Field(..., min_length=8)\n",
    "\n",
    "class UserResponse(BaseModel):\n",
    "    id: int\n",
    "    username: str\n",
    "    email: str\n",
    "    \n",
    "    class Config:\n",
    "        from_attributes = True\n",
    "\n",
    "# Endpoints\n",
    "@app.post(\"/users\", response_model=UserResponse, status_code=status.HTTP_201_CREATED)\n",
    "async def create_user(\n",
    "    user: UserCreate,\n",
    "    db = Depends(get_db)\n",
    "):\n",
    "    # Create user logic\n",
    "    return UserResponse(id=1, username=user.username, email=user.email)\n",
    "\n",
    "@app.get(\"/users/me\", response_model=UserResponse)\n",
    "async def get_me(current_user = Depends(get_current_user)):\n",
    "    return current_user\n",
    "'''\n",
    "\n",
    "print(\"FastAPI Production Code:\")\n",
    "print(fastapi_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Database & ORM\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| SQLAlchemy 2.0 | Modern async ORM | â¬œ |\n",
    "| Alembic | Database migrations | â¬œ |\n",
    "| Connection Pooling | Performance | â¬œ |\n",
    "| Query Optimization | N+1, indexes | â¬œ |\n",
    "| Transactions | ACID compliance | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLAlchemy 2.0 async example\n",
    "sqlalchemy_code = '''\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker\n",
    "from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship\n",
    "from sqlalchemy import ForeignKey, select, func\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# Base class\n",
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "# Models\n",
    "class User(Base):\n",
    "    __tablename__ = \"users\"\n",
    "    \n",
    "    id: Mapped[int] = mapped_column(primary_key=True)\n",
    "    email: Mapped[str] = mapped_column(unique=True, index=True)\n",
    "    hashed_password: Mapped[str]\n",
    "    is_active: Mapped[bool] = mapped_column(default=True)\n",
    "    created_at: Mapped[datetime] = mapped_column(default=datetime.utcnow)\n",
    "    \n",
    "    posts: Mapped[List[\"Post\"]] = relationship(back_populates=\"author\", lazy=\"selectin\")\n",
    "\n",
    "class Post(Base):\n",
    "    __tablename__ = \"posts\"\n",
    "    \n",
    "    id: Mapped[int] = mapped_column(primary_key=True)\n",
    "    title: Mapped[str] = mapped_column(index=True)\n",
    "    content: Mapped[str]\n",
    "    author_id: Mapped[int] = mapped_column(ForeignKey(\"users.id\"))\n",
    "    \n",
    "    author: Mapped[\"User\"] = relationship(back_populates=\"posts\")\n",
    "\n",
    "# Async engine with connection pooling\n",
    "engine = create_async_engine(\n",
    "    \"postgresql+asyncpg://user:pass@localhost/db\",\n",
    "    pool_size=20,\n",
    "    max_overflow=10,\n",
    "    pool_pre_ping=True,\n",
    "    echo=False\n",
    ")\n",
    "\n",
    "async_session = async_sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n",
    "\n",
    "# Repository pattern\n",
    "class UserRepository:\n",
    "    def __init__(self, session: AsyncSession):\n",
    "        self.session = session\n",
    "    \n",
    "    async def get_by_id(self, user_id: int) -> Optional[User]:\n",
    "        result = await self.session.execute(\n",
    "            select(User).where(User.id == user_id)\n",
    "        )\n",
    "        return result.scalar_one_or_none()\n",
    "    \n",
    "    async def get_with_posts(self, user_id: int) -> Optional[User]:\n",
    "        result = await self.session.execute(\n",
    "            select(User)\n",
    "            .options(selectinload(User.posts))\n",
    "            .where(User.id == user_id)\n",
    "        )\n",
    "        return result.scalar_one_or_none()\n",
    "    \n",
    "    async def create(self, user: User) -> User:\n",
    "        self.session.add(user)\n",
    "        await self.session.commit()\n",
    "        await self.session.refresh(user)\n",
    "        return user\n",
    "'''\n",
    "\n",
    "print(\"SQLAlchemy 2.0 Async:\")\n",
    "print(sqlalchemy_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='phase-5-data-ml'></a>\n",
    "# ğŸ“Š Phase 5: Data Engineering & ML (Month 8)\n",
    "\n",
    "**Goal**: Build data pipelines and ML systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Data Processing at Scale\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Pandas Optimization | Memory, vectorization | â¬œ |\n",
    "| Polars | Fast alternative | â¬œ |\n",
    "| Dask/Ray | Distributed computing | â¬œ |\n",
    "| Apache Spark | PySpark | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pandas optimization techniques\n",
    "def optimize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Optimize DataFrame memory usage.\"\"\"\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type == 'object':\n",
    "            # Convert to category if low cardinality\n",
    "            if df[col].nunique() / len(df) < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "        \n",
    "        elif col_type == 'int64':\n",
    "            # Downcast integers\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        \n",
    "        elif col_type == 'float64':\n",
    "            # Downcast floats\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': range(100000),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 100000),\n",
    "    'value': np.random.randn(100000)\n",
    "})\n",
    "\n",
    "print(f\"Before optimization: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "df = optimize_dataframe(df)\n",
    "print(f\"After optimization: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 ML Pipeline & MLOps\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Scikit-learn Pipelines | Feature engineering | â¬œ |\n",
    "| MLflow | Experiment tracking | â¬œ |\n",
    "| Model Registry | Versioning | â¬œ |\n",
    "| Model Serving | BentoML, FastAPI | â¬œ |\n",
    "| Monitoring | Drift detection | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Pipeline example\n",
    "ml_pipeline_code = '''\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Define preprocessing\n",
    "numeric_features = [\"age\", \"salary\"]\n",
    "categorical_features = [\"department\", \"location\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "\n",
    "# MLflow tracking\n",
    "mlflow.set_experiment(\"employee-churn\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"n_estimators\": 100,\n",
    "        \"numeric_features\": numeric_features,\n",
    "        \"categorical_features\": categorical_features\n",
    "    })\n",
    "    \n",
    "    # Train and evaluate\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring=\"accuracy\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy_mean\", scores.mean())\n",
    "    mlflow.log_metric(\"accuracy_std\", scores.std())\n",
    "    \n",
    "    # Train final model\n",
    "    pipeline.fit(X, y)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(pipeline, \"model\")\n",
    "'''\n",
    "\n",
    "print(\"ML Pipeline with MLflow:\")\n",
    "print(ml_pipeline_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='phase-6-performance'></a>\n",
    "# ğŸš€ Phase 6: Performance & Optimization (Month 9)\n",
    "\n",
    "**Goal**: Senior-level performance optimization and profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 ğŸ”¥ Caching Strategies (Senior Level)\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| In-Memory Cache | LRU, TTL caching | â¬œ |\n",
    "| Redis | Distributed caching | â¬œ |\n",
    "| Cache Patterns | Cache-aside, write-through | â¬œ |\n",
    "| Cache Invalidation | Strategies | â¬œ |\n",
    "| Memoization | Function result caching | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hashlib\n",
    "from functools import wraps\n",
    "from typing import Any, Optional, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from collections import OrderedDict\n",
    "import threading\n",
    "\n",
    "@dataclass\n",
    "class CacheEntry:\n",
    "    value: Any\n",
    "    expires_at: float\n",
    "    created_at: float = field(default_factory=time.time)\n",
    "\n",
    "class TTLCache:\n",
    "    \"\"\"Thread-safe TTL cache with LRU eviction.\"\"\"\n",
    "    \n",
    "    def __init__(self, maxsize: int = 1000, default_ttl: float = 300):\n",
    "        self.maxsize = maxsize\n",
    "        self.default_ttl = default_ttl\n",
    "        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()\n",
    "        self._lock = threading.RLock()\n",
    "        self._hits = 0\n",
    "        self._misses = 0\n",
    "    \n",
    "    def get(self, key: str) -> Optional[Any]:\n",
    "        with self._lock:\n",
    "            if key not in self._cache:\n",
    "                self._misses += 1\n",
    "                return None\n",
    "            \n",
    "            entry = self._cache[key]\n",
    "            if time.time() > entry.expires_at:\n",
    "                del self._cache[key]\n",
    "                self._misses += 1\n",
    "                return None\n",
    "            \n",
    "            # Move to end (most recently used)\n",
    "            self._cache.move_to_end(key)\n",
    "            self._hits += 1\n",
    "            return entry.value\n",
    "    \n",
    "    def set(self, key: str, value: Any, ttl: Optional[float] = None) -> None:\n",
    "        with self._lock:\n",
    "            ttl = ttl or self.default_ttl\n",
    "            expires_at = time.time() + ttl\n",
    "            \n",
    "            if key in self._cache:\n",
    "                self._cache.move_to_end(key)\n",
    "            \n",
    "            self._cache[key] = CacheEntry(value=value, expires_at=expires_at)\n",
    "            \n",
    "            # Evict oldest if over capacity\n",
    "            while len(self._cache) > self.maxsize:\n",
    "                self._cache.popitem(last=False)\n",
    "    \n",
    "    def delete(self, key: str) -> bool:\n",
    "        with self._lock:\n",
    "            if key in self._cache:\n",
    "                del self._cache[key]\n",
    "                return True\n",
    "            return False\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        with self._lock:\n",
    "            self._cache.clear()\n",
    "    \n",
    "    @property\n",
    "    def stats(self) -> dict:\n",
    "        with self._lock:\n",
    "            total = self._hits + self._misses\n",
    "            hit_rate = self._hits / total if total > 0 else 0\n",
    "            return {\n",
    "                \"size\": len(self._cache),\n",
    "                \"hits\": self._hits,\n",
    "                \"misses\": self._misses,\n",
    "                \"hit_rate\": f\"{hit_rate:.2%}\"\n",
    "            }\n",
    "\n",
    "# Decorator for function caching\n",
    "def cached(cache: TTLCache, ttl: Optional[float] = None, key_prefix: str = \"\"):\n",
    "    \"\"\"Decorator for caching function results.\"\"\"\n",
    "    def decorator(func: Callable) -> Callable:\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Generate cache key\n",
    "            key_data = f\"{key_prefix}:{func.__name__}:{args}:{sorted(kwargs.items())}\"\n",
    "            cache_key = hashlib.md5(key_data.encode()).hexdigest()\n",
    "            \n",
    "            # Try to get from cache\n",
    "            result = cache.get(cache_key)\n",
    "            if result is not None:\n",
    "                return result\n",
    "            \n",
    "            # Execute function and cache result\n",
    "            result = func(*args, **kwargs)\n",
    "            cache.set(cache_key, result, ttl)\n",
    "            return result\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Usage example\n",
    "cache = TTLCache(maxsize=100, default_ttl=60)\n",
    "\n",
    "@cached(cache, ttl=30)\n",
    "def expensive_computation(n: int) -> int:\n",
    "    time.sleep(0.1)  # Simulate expensive operation\n",
    "    return n ** 2\n",
    "\n",
    "# First call - computes\n",
    "start = time.time()\n",
    "result1 = expensive_computation(10)\n",
    "print(f\"First call: {result1}, took {time.time() - start:.3f}s\")\n",
    "\n",
    "# Second call - cached\n",
    "start = time.time()\n",
    "result2 = expensive_computation(10)\n",
    "print(f\"Second call: {result2}, took {time.time() - start:.6f}s\")\n",
    "\n",
    "print(f\"Cache stats: {cache.stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redis caching example\n",
    "redis_cache_code = '''\n",
    "import redis\n",
    "import json\n",
    "from typing import Any, Optional\n",
    "from functools import wraps\n",
    "\n",
    "class RedisCache:\n",
    "    \"\"\"Production Redis cache with patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, host: str = \"localhost\", port: int = 6379, db: int = 0):\n",
    "        self.client = redis.Redis(\n",
    "            host=host, \n",
    "            port=port, \n",
    "            db=db,\n",
    "            decode_responses=True,\n",
    "            socket_connect_timeout=5,\n",
    "            socket_keepalive=True\n",
    "        )\n",
    "    \n",
    "    def get(self, key: str) -> Optional[Any]:\n",
    "        value = self.client.get(key)\n",
    "        return json.loads(value) if value else None\n",
    "    \n",
    "    def set(self, key: str, value: Any, ttl: int = 300) -> bool:\n",
    "        return self.client.setex(key, ttl, json.dumps(value))\n",
    "    \n",
    "    def delete(self, key: str) -> int:\n",
    "        return self.client.delete(key)\n",
    "    \n",
    "    def delete_pattern(self, pattern: str) -> int:\n",
    "        \"\"\"Delete all keys matching pattern.\"\"\"\n",
    "        keys = self.client.keys(pattern)\n",
    "        if keys:\n",
    "            return self.client.delete(*keys)\n",
    "        return 0\n",
    "    \n",
    "    # Cache-aside pattern\n",
    "    def cache_aside(self, key: str, loader, ttl: int = 300) -> Any:\n",
    "        \"\"\"Get from cache or load and cache.\"\"\"\n",
    "        value = self.get(key)\n",
    "        if value is None:\n",
    "            value = loader()\n",
    "            self.set(key, value, ttl)\n",
    "        return value\n",
    "    \n",
    "    # Write-through pattern\n",
    "    def write_through(self, key: str, value: Any, writer, ttl: int = 300) -> Any:\n",
    "        \"\"\"Write to both cache and storage.\"\"\"\n",
    "        writer(value)  # Write to database first\n",
    "        self.set(key, value, ttl)\n",
    "        return value\n",
    "\n",
    "# Usage\n",
    "cache = RedisCache()\n",
    "\n",
    "# Cache-aside pattern\n",
    "def get_user(user_id: int) -> dict:\n",
    "    return cache.cache_aside(\n",
    "        f\"user:{user_id}\",\n",
    "        lambda: db.get_user(user_id),  # Load from DB if not cached\n",
    "        ttl=600\n",
    "    )\n",
    "\n",
    "# Invalidation on update\n",
    "def update_user(user_id: int, data: dict):\n",
    "    db.update_user(user_id, data)\n",
    "    cache.delete(f\"user:{user_id}\")\n",
    "    cache.delete_pattern(f\"user:{user_id}:*\")  # Invalidate related cache\n",
    "'''\n",
    "\n",
    "print(\"Redis Cache with Patterns:\")\n",
    "print(redis_cache_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Profiling & Optimization\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| cProfile | CPU profiling | â¬œ |\n",
    "| line_profiler | Line-by-line profiling | â¬œ |\n",
    "| memory_profiler | Memory profiling | â¬œ |\n",
    "| py-spy | Sampling profiler | â¬œ |\n",
    "| Optimization Patterns | Common optimizations | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def profile(sort_by='cumulative', lines=20):\n",
    "    \"\"\"Context manager for profiling code.\"\"\"\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        profiler.disable()\n",
    "        stream = io.StringIO()\n",
    "        stats = pstats.Stats(profiler, stream=stream).sort_stats(sort_by)\n",
    "        stats.print_stats(lines)\n",
    "        print(stream.getvalue())\n",
    "\n",
    "# Example usage\n",
    "def slow_function():\n",
    "    result = []\n",
    "    for i in range(10000):\n",
    "        result.append(i ** 2)\n",
    "    return result\n",
    "\n",
    "def fast_function():\n",
    "    return [i ** 2 for i in range(10000)]\n",
    "\n",
    "with profile(lines=5):\n",
    "    slow_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization with __slots__\n",
    "import sys\n",
    "\n",
    "class RegularUser:\n",
    "    def __init__(self, name, email, age):\n",
    "        self.name = name\n",
    "        self.email = email\n",
    "        self.age = age\n",
    "\n",
    "class OptimizedUser:\n",
    "    __slots__ = ['name', 'email', 'age']\n",
    "    \n",
    "    def __init__(self, name, email, age):\n",
    "        self.name = name\n",
    "        self.email = email\n",
    "        self.age = age\n",
    "\n",
    "# Compare memory usage\n",
    "regular = RegularUser(\"John\", \"john@email.com\", 30)\n",
    "optimized = OptimizedUser(\"John\", \"john@email.com\", 30)\n",
    "\n",
    "print(f\"Regular User size: {sys.getsizeof(regular)} + {sys.getsizeof(regular.__dict__)} (dict)\")\n",
    "print(f\"Optimized User size: {sys.getsizeof(optimized)} (no __dict__)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='phase-7-system-design'></a>\n",
    "# ğŸ—ï¸ Phase 7: System Design & Architecture (Month 10)\n",
    "\n",
    "**Goal**: Design scalable, maintainable systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Design Patterns\n",
    "\n",
    "| Pattern | Description | Status |\n",
    "|---------|-------------|--------|\n",
    "| Singleton | Single instance | â¬œ |\n",
    "| Factory | Object creation | â¬œ |\n",
    "| Strategy | Interchangeable algorithms | â¬œ |\n",
    "| Observer | Event handling | â¬œ |\n",
    "| Repository | Data access abstraction | â¬œ |\n",
    "| Unit of Work | Transaction management | â¬œ |\n",
    "| CQRS | Command Query Separation | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Callable, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# Strategy Pattern\n",
    "class PaymentStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def pay(self, amount: float) -> bool:\n",
    "        pass\n",
    "\n",
    "class CreditCardPayment(PaymentStrategy):\n",
    "    def __init__(self, card_number: str):\n",
    "        self.card_number = card_number\n",
    "    \n",
    "    def pay(self, amount: float) -> bool:\n",
    "        print(f\"Paying ${amount} with credit card {self.card_number[-4:]}\")\n",
    "        return True\n",
    "\n",
    "class PayPalPayment(PaymentStrategy):\n",
    "    def __init__(self, email: str):\n",
    "        self.email = email\n",
    "    \n",
    "    def pay(self, amount: float) -> bool:\n",
    "        print(f\"Paying ${amount} with PayPal ({self.email})\")\n",
    "        return True\n",
    "\n",
    "class PaymentProcessor:\n",
    "    def __init__(self, strategy: PaymentStrategy):\n",
    "        self._strategy = strategy\n",
    "    \n",
    "    def process_payment(self, amount: float) -> bool:\n",
    "        return self._strategy.pay(amount)\n",
    "\n",
    "# Observer Pattern\n",
    "class EventEmitter:\n",
    "    def __init__(self):\n",
    "        self._listeners: Dict[str, List[Callable]] = {}\n",
    "    \n",
    "    def on(self, event: str, callback: Callable) -> None:\n",
    "        if event not in self._listeners:\n",
    "            self._listeners[event] = []\n",
    "        self._listeners[event].append(callback)\n",
    "    \n",
    "    def emit(self, event: str, *args, **kwargs) -> None:\n",
    "        for callback in self._listeners.get(event, []):\n",
    "            callback(*args, **kwargs)\n",
    "    \n",
    "    def off(self, event: str, callback: Callable) -> None:\n",
    "        if event in self._listeners:\n",
    "            self._listeners[event].remove(callback)\n",
    "\n",
    "# Usage\n",
    "processor = PaymentProcessor(CreditCardPayment(\"1234-5678-9012-3456\"))\n",
    "processor.process_payment(99.99)\n",
    "\n",
    "emitter = EventEmitter()\n",
    "emitter.on(\"order_created\", lambda order: print(f\"Order created: {order}\"))\n",
    "emitter.on(\"order_created\", lambda order: print(f\"Sending email for: {order}\"))\n",
    "emitter.emit(\"order_created\", {\"id\": 1, \"total\": 99.99})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Message Queues & Event-Driven\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Celery | Task queues | â¬œ |\n",
    "| RabbitMQ | Message broker | â¬œ |\n",
    "| Kafka | Event streaming | â¬œ |\n",
    "| Event Sourcing | Event-driven architecture | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celery task example\n",
    "celery_code = '''\n",
    "from celery import Celery, Task\n",
    "from celery.exceptions import MaxRetriesExceededError\n",
    "import logging\n",
    "\n",
    "app = Celery(\n",
    "    \"tasks\",\n",
    "    broker=\"redis://localhost:6379/0\",\n",
    "    backend=\"redis://localhost:6379/1\"\n",
    ")\n",
    "\n",
    "app.conf.update(\n",
    "    task_serializer=\"json\",\n",
    "    accept_content=[\"json\"],\n",
    "    result_serializer=\"json\",\n",
    "    timezone=\"UTC\",\n",
    "    enable_utc=True,\n",
    "    task_track_started=True,\n",
    "    task_time_limit=30 * 60,  # 30 minutes\n",
    "    worker_prefetch_multiplier=1,\n",
    ")\n",
    "\n",
    "# Base task with error handling\n",
    "class BaseTask(Task):\n",
    "    abstract = True\n",
    "    \n",
    "    def on_failure(self, exc, task_id, args, kwargs, einfo):\n",
    "        logging.error(f\"Task {task_id} failed: {exc}\")\n",
    "    \n",
    "    def on_success(self, retval, task_id, args, kwargs):\n",
    "        logging.info(f\"Task {task_id} completed successfully\")\n",
    "\n",
    "@app.task(base=BaseTask, bind=True, max_retries=3)\n",
    "def process_order(self, order_id: int):\n",
    "    try:\n",
    "        # Process order\n",
    "        order = get_order(order_id)\n",
    "        process_payment(order)\n",
    "        send_confirmation_email(order)\n",
    "        return {\"status\": \"success\", \"order_id\": order_id}\n",
    "    except TransientError as exc:\n",
    "        # Retry with exponential backoff\n",
    "        raise self.retry(exc=exc, countdown=2 ** self.request.retries)\n",
    "    except Exception as exc:\n",
    "        logging.error(f\"Order processing failed: {exc}\")\n",
    "        raise\n",
    "\n",
    "# Chain tasks\n",
    "from celery import chain, group\n",
    "\n",
    "workflow = chain(\n",
    "    validate_order.s(order_id),\n",
    "    process_payment.s(),\n",
    "    fulfill_order.s(),\n",
    "    send_notification.s()\n",
    ")\n",
    "\n",
    "result = workflow.apply_async()\n",
    "'''\n",
    "\n",
    "print(\"Celery Task Queue:\")\n",
    "print(celery_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='phase-8-production'></a>\n",
    "# ğŸš¢ Phase 8: Production & DevOps (Month 11)\n",
    "\n",
    "**Goal**: Deploy and operate production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Containerization & Orchestration\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Docker | Multi-stage builds | â¬œ |\n",
    "| Docker Compose | Local orchestration | â¬œ |\n",
    "| Kubernetes | Container orchestration | â¬œ |\n",
    "| Helm | K8s package manager | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Dockerfile\n",
    "dockerfile = '''\n",
    "# Build stage\n",
    "FROM python:3.11-slim as builder\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install build dependencies\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\\\n",
    "    build-essential \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt\n",
    "\n",
    "# Production stage\n",
    "FROM python:3.11-slim as production\n",
    "\n",
    "# Create non-root user\n",
    "RUN useradd --create-home appuser\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy wheels from builder\n",
    "COPY --from=builder /app/wheels /wheels\n",
    "RUN pip install --no-cache /wheels/*\n",
    "\n",
    "# Copy application code\n",
    "COPY --chown=appuser:appuser ./app ./app\n",
    "\n",
    "# Switch to non-root user\n",
    "USER appuser\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Run application\n",
    "EXPOSE 8000\n",
    "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "'''\n",
    "\n",
    "print(\"Production Dockerfile:\")\n",
    "print(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Monitoring & Observability\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Prometheus | Metrics collection | â¬œ |\n",
    "| Grafana | Visualization | â¬œ |\n",
    "| OpenTelemetry | Distributed tracing | â¬œ |\n",
    "| Sentry | Error tracking | â¬œ |\n",
    "| ELK Stack | Log aggregation | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prometheus metrics example\n",
    "monitoring_code = '''\n",
    "from prometheus_client import Counter, Histogram, Gauge, generate_latest\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import Response\n",
    "import time\n",
    "\n",
    "# Define metrics\n",
    "REQUEST_COUNT = Counter(\n",
    "    \"http_requests_total\",\n",
    "    \"Total HTTP requests\",\n",
    "    [\"method\", \"endpoint\", \"status\"]\n",
    ")\n",
    "\n",
    "REQUEST_LATENCY = Histogram(\n",
    "    \"http_request_duration_seconds\",\n",
    "    \"HTTP request latency\",\n",
    "    [\"method\", \"endpoint\"],\n",
    "    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]\n",
    ")\n",
    "\n",
    "ACTIVE_REQUESTS = Gauge(\n",
    "    \"http_requests_active\",\n",
    "    \"Number of active HTTP requests\"\n",
    ")\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def metrics_middleware(request: Request, call_next):\n",
    "    ACTIVE_REQUESTS.inc()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = await call_next(request)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    REQUEST_COUNT.labels(\n",
    "        method=request.method,\n",
    "        endpoint=request.url.path,\n",
    "        status=response.status_code\n",
    "    ).inc()\n",
    "    \n",
    "    REQUEST_LATENCY.labels(\n",
    "        method=request.method,\n",
    "        endpoint=request.url.path\n",
    "    ).observe(duration)\n",
    "    \n",
    "    ACTIVE_REQUESTS.dec()\n",
    "    return response\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    return Response(generate_latest(), media_type=\"text/plain\")\n",
    "'''\n",
    "\n",
    "print(\"Prometheus Metrics:\")\n",
    "print(monitoring_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='phase-9-leadership'></a>\n",
    "# ğŸ‘” Phase 9: Tech Lead Skills (Month 12)\n",
    "\n",
    "**Goal**: Lead teams and make technical decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Code Review & Standards\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| Code Review Best Practices | Effective reviews | â¬œ |\n",
    "| Style Guides | PEP 8, team standards | â¬œ |\n",
    "| Architecture Decision Records | ADRs | â¬œ |\n",
    "| Technical Documentation | System docs | â¬œ |\n",
    "| Mentoring | Growing team members | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Review Checklist\n",
    "code_review_checklist = '''\n",
    "# Code Review Checklist for Tech Leads\n",
    "\n",
    "## Functionality\n",
    "- [ ] Does the code do what it's supposed to do?\n",
    "- [ ] Are edge cases handled?\n",
    "- [ ] Is error handling appropriate?\n",
    "\n",
    "## Code Quality\n",
    "- [ ] Is the code readable and self-documenting?\n",
    "- [ ] Are functions/methods focused and small?\n",
    "- [ ] Is there code duplication that should be refactored?\n",
    "- [ ] Are variable/function names descriptive?\n",
    "\n",
    "## Architecture\n",
    "- [ ] Does this follow our architectural patterns?\n",
    "- [ ] Is the code in the right layer/module?\n",
    "- [ ] Are dependencies appropriate?\n",
    "\n",
    "## Testing\n",
    "- [ ] Are there sufficient unit tests?\n",
    "- [ ] Are edge cases tested?\n",
    "- [ ] Do tests follow AAA pattern (Arrange, Act, Assert)?\n",
    "\n",
    "## Performance\n",
    "- [ ] Are there any obvious performance issues?\n",
    "- [ ] Is caching used appropriately?\n",
    "- [ ] Are database queries optimized?\n",
    "\n",
    "## Security\n",
    "- [ ] Is user input validated?\n",
    "- [ ] Are there SQL injection risks?\n",
    "- [ ] Are secrets properly managed?\n",
    "\n",
    "## Documentation\n",
    "- [ ] Are complex algorithms documented?\n",
    "- [ ] Are public APIs documented?\n",
    "- [ ] Is the README updated if needed?\n",
    "'''\n",
    "\n",
    "print(code_review_checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Architecture Decision Records (ADR)\n",
    "\n",
    "| Topic | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| ADR Format | Structure and content | â¬œ |\n",
    "| Decision Making | Evaluating trade-offs | â¬œ |\n",
    "| Documentation | Recording decisions | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADR Template\n",
    "adr_template = '''\n",
    "# ADR-001: Use PostgreSQL as Primary Database\n",
    "\n",
    "## Status\n",
    "Accepted\n",
    "\n",
    "## Context\n",
    "We need to choose a database for our new microservice. The service will:\n",
    "- Handle ~10,000 transactions per second\n",
    "- Store structured data with complex relationships\n",
    "- Require ACID compliance\n",
    "- Need full-text search capabilities\n",
    "\n",
    "## Decision\n",
    "We will use PostgreSQL 15 as our primary database.\n",
    "\n",
    "## Rationale\n",
    "- ACID compliant for data integrity\n",
    "- Excellent performance for read-heavy workloads\n",
    "- Built-in full-text search\n",
    "- Strong JSON support for flexible schema needs\n",
    "- Team expertise exists\n",
    "- Mature ecosystem and tooling\n",
    "\n",
    "## Alternatives Considered\n",
    "\n",
    "### MySQL\n",
    "- Pros: Familiar, good performance\n",
    "- Cons: Less robust JSON support, weaker full-text search\n",
    "\n",
    "### MongoDB\n",
    "- Pros: Flexible schema, horizontal scaling\n",
    "- Cons: No ACID across documents, team unfamiliar\n",
    "\n",
    "## Consequences\n",
    "\n",
    "### Positive\n",
    "- Reliable ACID transactions\n",
    "- Strong query capabilities\n",
    "- Good scaling with read replicas\n",
    "\n",
    "### Negative\n",
    "- Horizontal scaling more complex than NoSQL\n",
    "- Requires schema migrations\n",
    "\n",
    "## References\n",
    "- PostgreSQL Documentation: https://www.postgresql.org/docs/\n",
    "- Performance benchmarks: [internal doc link]\n",
    "'''\n",
    "\n",
    "print(adr_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Technical Leadership Skills\n",
    "\n",
    "| Skill | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| System Design Interviews | Whiteboard design | â¬œ |\n",
    "| Capacity Planning | Scaling estimates | â¬œ |\n",
    "| Technical Debt Management | Prioritization | â¬œ |\n",
    "| Incident Response | On-call, postmortems | â¬œ |\n",
    "| Stakeholder Communication | Technical translation | â¬œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Design Template\n",
    "system_design_template = '''\n",
    "# System Design: URL Shortener\n",
    "\n",
    "## 1. Requirements Clarification\n",
    "\n",
    "### Functional Requirements\n",
    "- Shorten long URLs\n",
    "- Redirect short URLs to original\n",
    "- Custom short URLs (optional)\n",
    "- Analytics (clicks, geography)\n",
    "\n",
    "### Non-Functional Requirements\n",
    "- High availability (99.9%)\n",
    "- Low latency (<100ms redirect)\n",
    "- Scale: 100M URLs, 10B redirects/month\n",
    "\n",
    "## 2. Capacity Estimation\n",
    "\n",
    "### Traffic\n",
    "- Writes: 100M URLs / 30 days = ~40 URLs/sec\n",
    "- Reads: 10B / 30 days = ~4000 redirects/sec\n",
    "- Read:Write ratio = 100:1 (read-heavy)\n",
    "\n",
    "### Storage\n",
    "- Per URL: ~500 bytes\n",
    "- 100M URLs = 50GB\n",
    "- 5 years: 250GB (manageable)\n",
    "\n",
    "## 3. High-Level Design\n",
    "\n",
    "[Client] -> [Load Balancer] -> [API Servers] -> [Cache] -> [Database]\n",
    "                                     |\n",
    "                              [Key Generation Service]\n",
    "\n",
    "## 4. Database Design\n",
    "\n",
    "urls table:\n",
    "- short_code (PK, VARCHAR(7))\n",
    "- original_url (TEXT)\n",
    "- user_id (FK)\n",
    "- created_at (TIMESTAMP)\n",
    "- expires_at (TIMESTAMP)\n",
    "\n",
    "## 5. Key Generation\n",
    "\n",
    "Options:\n",
    "1. Base62 encoding of auto-increment ID\n",
    "2. Pre-generated keys (Key DB)\n",
    "3. Hash + collision handling\n",
    "\n",
    "Choice: Pre-generated keys for performance\n",
    "\n",
    "## 6. Caching Strategy\n",
    "\n",
    "- Redis cache for hot URLs\n",
    "- LRU eviction\n",
    "- 20% cache hit rate can reduce DB load significantly\n",
    "\n",
    "## 7. Scaling Considerations\n",
    "\n",
    "- Database: Read replicas, sharding by short_code\n",
    "- Cache: Redis Cluster\n",
    "- API: Horizontal scaling, stateless\n",
    "'''\n",
    "\n",
    "print(system_design_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“… Complete Learning Timeline\n",
    "\n",
    "| Month | Phase | Level | Key Topics |\n",
    "|-------|-------|-------|------------|\n",
    "| 1-2 | Foundations | Junior | Syntax, Data Structures, Control Flow |\n",
    "| 3-4 | Intermediate | Junior+ | OOP, Testing, File Handling, Error Handling |\n",
    "| 5-6 | Advanced | Mid | Decorators, Generators, Threading, Async |\n",
    "| 7 | Web & APIs | Mid | FastAPI, SQLAlchemy, REST APIs |\n",
    "| 8 | Data/ML | Mid+ | Pandas, ML Pipelines, MLOps |\n",
    "| 9 | Performance | Senior | Caching, Profiling, Optimization |\n",
    "| 10 | Architecture | Senior | Design Patterns, Message Queues, System Design |\n",
    "| 11 | Production | Senior+ | Docker, K8s, CI/CD, Monitoring |\n",
    "| 12 | Leadership | Tech Lead | Code Review, ADRs, Team Leadership |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Progress Tracker\n",
    "phases = {\n",
    "    \"Phase 1: Fundamentals\": {\"total\": 20, \"completed\": 0},\n",
    "    \"Phase 2: Intermediate\": {\"total\": 25, \"completed\": 0},\n",
    "    \"Phase 3: Advanced (Threading/Async)\": {\"total\": 30, \"completed\": 0},\n",
    "    \"Phase 4: Web & APIs\": {\"total\": 20, \"completed\": 0},\n",
    "    \"Phase 5: Data & ML\": {\"total\": 20, \"completed\": 0},\n",
    "    \"Phase 6: Performance (Caching)\": {\"total\": 25, \"completed\": 0},\n",
    "    \"Phase 7: System Design\": {\"total\": 20, \"completed\": 0},\n",
    "    \"Phase 8: Production/DevOps\": {\"total\": 20, \"completed\": 0},\n",
    "    \"Phase 9: Tech Lead Skills\": {\"total\": 15, \"completed\": 0},\n",
    "}\n",
    "\n",
    "total_topics = sum(p[\"total\"] for p in phases.values())\n",
    "completed_topics = sum(p[\"completed\"] for p in phases.values())\n",
    "\n",
    "print(\"ğŸ Python: Beginner to Tech Lead - Progress Tracker\")\n",
    "print(\"=\" * 65)\n",
    "for phase, data in phases.items():\n",
    "    pct = (data[\"completed\"] / data[\"total\"]) * 100\n",
    "    bar = \"â–ˆ\" * int(pct // 5) + \"â–‘\" * (20 - int(pct // 5))\n",
    "    print(f\"{phase:40} [{bar}] {pct:5.1f}%\")\n",
    "\n",
    "print(\"=\" * 65)\n",
    "overall_pct = (completed_topics / total_topics) * 100\n",
    "print(f\"{'Overall Progress':40} {completed_topics}/{total_topics} ({overall_pct:.1f}%)\")\n",
    "print(f\"\\nğŸ¯ Target: Junior â†’ Mid â†’ Senior â†’ Tech Lead in 12 months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“š Resources by Level\n",
    "\n",
    "## Junior Level\n",
    "- **Books**: Python Crash Course, Automate the Boring Stuff\n",
    "- **Practice**: LeetCode Easy, HackerRank Python\n",
    "\n",
    "## Mid Level\n",
    "- **Books**: Fluent Python, Effective Python, Python Cookbook\n",
    "- **Practice**: LeetCode Medium, System Design Primer\n",
    "\n",
    "## Senior Level\n",
    "- **Books**: High Performance Python, Architecture Patterns with Python\n",
    "- **Practice**: System design interviews, open source contributions\n",
    "\n",
    "## Tech Lead\n",
    "- **Books**: The Manager's Path, Staff Engineer, Designing Data-Intensive Applications\n",
    "- **Practice**: Lead projects, mentor juniors, write ADRs\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ Key Takeaways\n",
    "\n",
    "1. **Master the fundamentals** - Strong basics make advanced topics easier\n",
    "2. **Build real projects** - Theory without practice is incomplete\n",
    "3. **Learn system design** - Essential for senior/lead roles\n",
    "4. **Practice code reviews** - Both giving and receiving feedback\n",
    "5. **Document decisions** - ADRs are crucial for team scaling\n",
    "6. **Never stop learning** - Technology evolves constantly\n",
    "\n",
    "**Good luck on your journey from Beginner to Tech Lead! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
